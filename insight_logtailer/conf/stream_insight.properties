# encoding: utf-8
# @author PangBo <pangbo@domob.cn>

###########################################################################
# 全局设置
###########################################################################

# stream的名称
# 名称应该只包含英文字母和数字
stream.name=insight_stream

# 数据来源的类型
# 1.text       普通文本文件，按照"\n"分割记录。一行的最大长度是16k(包含\n)，如果超长，会被分为多条记录
# 2.rawthrift  使用thrift二进制协议直接读取文件
# 3.kafka      kafka的某个topic
# 4.inline     使用特定的类处理消息, 该类需要实现MessageReader接口, 参见 reader.inline.class
stream.reader=kafka

# 使用特定的发送方式对日志文件进行处理
# 1.streaming   发送到指定脚本的标准输入
# 2.hadoop      将消息发送到hadoop上，TODO
# 3.inline      使用特定的类处理消息，该类需要实现MessageSender接口
# 4.kafka       将message发送到kafka中
# 5.thrift      将消息发送到thrift服务，该服务需要实现LogProcessor处理器
stream.sender=streaming

# 设置这个流的观察者，写邮箱地址
# 例如： lizhengxu@domob.cn, someone@domob.cn
stream.watchers=

# 设置这个流的负责人，写邮箱地址
# 例如： lizhengxu@domob.cn, someone@domob.cn
stream.responsors= 

# 小时级低阈值，单位是 byte
# 如果这个线程在一个小时内读取字节数低于设定阈值，就会进行报警
# 例如：1024*1024*50，代表一小时内读取数据少于50MB时，进行报警
reader.low.threshold.hourly=

# 天级低阈值，单位是 byte
# 如果这个线程在一天内读取字节数低于设定阈值，就会进行报警
# 例如：1024*1024*1024*3，代表一天内读取数据少于3GB时，进行报警
reader.low.threshold.daily=

# 当sender错误时，最多重试的次数
# 不同类型的sender出现的错误可能不同
# 如kafka的sender可能是下游服务不可用，streaming的sender可能程序错误
# 当连续遇到如下指定的错误次数后，处理线程将停止，等待人工处理
# 注意：当sender为kafka，且为async时，这个参数无效。如果发送失败，则只会丢失这个数据，线程也不会停止。
# 如果是-1则不断重试，0则不重试。
# 默认是3，即重试3次。
sender.retry.on.error = 5

# 重试间隔时间（毫秒）
sender.retry.interval.ms = 3000

###########################################################################
# 以下是reader的配置，根据reader类型的不同，有效的配置项也会不同                     #
###########################################################################

#-----------------------------------------------------------------------------
## reader=kafka 时，以下项有效
#
reader.kafka.topic=qianji_nginx_data
reader.kafka.group.id=qianji_nginx_data

# 以下是kafka consumer api中指定的配置，去掉reader.kafka.前缀后传给Consumer
# zk地址如果不填，会连接线上线下默认的zk
# 一般情况用户无需进行填写
#reader.kafka.zookeeper.hosts=
reader.kafka.zookeeper.session.timeout.ms=20000
reader.kafka.zookeeper.sync.time.ms=20000

# 代表起几个consumer线程，从kafka读取消息。即，如果想启动俩个消费线程，这里设成2即可
# 注意： consumer线程必须拥有不同的 consumer.id(参考 reader.kafka.consumer.id)。可以设置为空，那么会自动生成随机的id
#        如果设置俩个consumer，那么第一个consumer的id会由 reader.kafka.consumer.id 决定
#                                  第二个consumer的id会由 reader.kafka.consumer.id.1 决定，以此类推
# 如果是第一个consumer，那么 stream_name 是 配置文件指定的名字
# 如果是第二个consumer，那么 stream_name 是 配置文件指定的名字_thread1，以此类推
# 默认值是 1
reader.kafka.num.consumer.fetchers=

# 代表consumer线程拥有的id，必须是唯一。如果多个consumer拥有相同consumer.id，那么这些consumer当中只会有一个正常工作，其余都会一直等待。
# 还可以通过此id，一定程度上控制partition对consumer的分配
# 默认会生成一个随机的id
reader.kafka.consumer.id=

# 多长时间提交一次offset
# 如果程序异常重启，上次提交offset之后consume的消息，将会被replay。
# 正常重启的情况下，不会出现replay的现象。logtailer已经在程序退出时正确的保存了offset
# 因此较短的提交时间，会产生较小的数据重复。我们设定为5s。
# 下游如果需要完全避免数据重复，则需要根据消息内容，自己判断是否有重复的数据
# kafka的默认值是20s
reader.kafka.auto.commit.interval.ms=5000

# 当无再有新的到来数据时，最多阻塞多长时间抛出 timeout exception。
# 当抛出异常时，logtailer会捕获并再次进行读取。
# 如果不进行设置，默认为-1，表示一直阻塞。
# 注意：如果设置为-1，当无再有新数据，并且当要关闭logtailer时，因为 kafka conmuser会一直阻塞，所以会无法正常关闭。
reader.kafka.consumer.timeout.ms=2000

# what to do if an offset if out of range.
# smallest: automatically reset the offset to the smallest offset
# largest: automatically reset the offset to the largest offset
# 其他值会抛出异常
# 默认是smallest，如果读取的topic已经积累的很多历史数据，在初次启动时将会得到大量数据
# 通常我们可能希望从最新的位置开始读取一个topic
# 这个值只在初次注册consumer，或者offset越界时有用
reader.kafka.auto.offset.reset=largest
#reader.kafka.auto.offset.reset=smallest

# 还可以增加 reader.kafka. 开头的其他 kafka api中约定的配置项
# 参考：
# reader.kafka.socket.timeout.ms=30000
# reader.kafka.socket.buffersize=xxx # default is 64k
# 

###########################################################################
# 以下是sender的配置
# 实际上这些项都以sender.起始比较好，目前没有冲突，暂时不加
###########################################################################
#-----------------------------------------------------------------------------
# ==== 以下是用于 stream.sender=streaming 处理方式的配制 ====
# 处理脚本，尽量写绝对路径，可以带参数
# 相对路径时，相对的是logtailer的运行时目录
streaming.processor=python /home/fanjianing/hackathon/insight-stream-reader/bin/start_up.py -b /home/fanjianing/hackathon/insight-stream-reader -d /home/fanjianing/hackathon/insight-stream-reader

# 如上程序的运行时当前目录，为空时是logtailer的运行目录
streaming.processor.runtime.cwd=/home/fanjianing/hackathon/insight-stream-reader

# logtailer会在启动时清空这两个文件，且在处理过程中不会切分文件
# 较长的输出，请在脚本中用日志机制实现。
# 处理脚本的标准输出保存的位置，为空时不保存
streaming.processor.stdout=/home/fanjianing/hackathon/insight-stream-reader/logs/insight.stdout

# 处理脚本的标准错误保存的位置，为空时不保存
streaming.processor.stderr=/home/fanjianing/hackathon/insight-stream-reader/logs/insight.err

# 在logtailer退出时，最多等待streaming进程多少ms，默认是2000
# logtailer在关闭时，会close掉streaming进程的stdin，streaming进程读到EOF后，应该及时退出
# logtailer关闭时不宜等待的太久。这里，如果子进程超过这个时间还没有退出，则logtailer kill之
# TODO: 尚未实现，logtailer会等着子进程退出再退出。
#streaming.join.delay.ms=2000

# 是否使用logtailer_types.Message thrift来包裹消息
# 对于文本格式，这个意义不大
# 对于原始数据是thrift或者其他二进制格式的，通过封装一层，可以避免错误的原始导致之后数据都错乱
# 默认是false
# streaming.wrap.message=true

